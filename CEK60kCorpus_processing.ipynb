{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "# Reads in all the files and concatenates them into one\n",
    "read_files = glob.glob(\"CEK60kCorpus_third_20060330/cek*.txt\")\n",
    "with open(\"data/CEK60kCorpus_combined.txt\", \"wb\") as outfile:\n",
    "    for f in read_files:\n",
    "        with open(f, \"rb\") as infile:\n",
    "            outfile.write(infile.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reads in files\n",
    "file = open('data/CEK60kCorpus_combined.txt', 'r', encoding='utf-8') \n",
    "page = file.read() \n",
    "file.close()\n",
    "page = page.split('\\n') # Creates list of sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "61853\n",
      "61853\n",
      "61853\n",
      "61853\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "numbers = []\n",
    "ch_sentences = []\n",
    "eng_sentences = []\n",
    "kor_sentences = []\n",
    "\n",
    "ch_word_to_sentencenum_dict = {}\n",
    "num_CEK60k_sentence_dict = {}\n",
    "\n",
    "def AddWordToDict(word_list, sentence_num, input_dict):\n",
    "    exclude = {'.':0, '':0,',':0}\n",
    "    for x in word_list:\n",
    "        if x not in exclude:\n",
    "            if input_dict.get(x) != None:\n",
    "                input_dict[x] += [sentence_num]\n",
    "            else:\n",
    "                input_dict[x] = [sentence_num]\n",
    "\n",
    "for i,x in enumerate(page):\n",
    "    if re.search('^[0-9]{1,5}:',x) != None:\n",
    "        # Chinese sentence\n",
    "        #chn = page[i].encode('l1').decode('gb2312') # For decoding mojibake\n",
    "        chn = page[i]\n",
    "        #print(chn)\n",
    "        \n",
    "        chn_split = chn.split(':')\n",
    "\n",
    "        chn_split[0] = chn_split[0].replace('\\ufeff','')\n",
    "        sentence_number = int(chn_split[0])\n",
    "        chn = chn_split[1] # Removes preceding number\n",
    "                \n",
    "        chn_segmented = chn.split(\"\\\\\") # List of segmented Chinese words\n",
    "        chn_segmented = list(set(chn_segmented))\n",
    "        AddWordToDict(chn_segmented, sentence_number, ch_word_to_sentencenum_dict) # Adds words to dictionary\n",
    "        \n",
    "        chn = chn.replace(\"\\\\\", \"\") # Removes backslashes to produce cleaned sentence\n",
    "        \n",
    "        # English sentence\n",
    "        eng = page[i+1]\n",
    "        eng = eng.replace(\"#\",'')\n",
    "        \n",
    "        # Korean sentence\n",
    "        #kor = page[i+2].encode('l1').decode('EUC-KR') # For decoding mojibake\n",
    "        kor = page[i+2]\n",
    "        kor = kor.replace(\"#\",'')\n",
    "        \n",
    "        # Appends numbers and sentences to lists\n",
    "        numbers.append(sentence_number)\n",
    "        ch_sentences.append(chn)\n",
    "        eng_sentences.append(eng)\n",
    "        kor_sentences.append(kor)\n",
    "        \n",
    "        # Adds sentences to dictionary\n",
    "        num_CEK60k_sentence_dict[sentence_number] = {'chn':chn, 'eng':eng ,'kor':kor}\n",
    "        #print(chn,eng,kor)\n",
    "        \n",
    "        \n",
    "print(len(numbers))\n",
    "print(len(ch_sentences))\n",
    "print(len(eng_sentences))\n",
    "print(len(kor_sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates a Pandas dataframe from data\n",
    "import pandas as pd\n",
    "df = pd.DataFrame({'id':numbers, 'Chinese':ch_sentences,'English':eng_sentences,'Korean':kor_sentences})\n",
    "df.sort_values(by=['id'])\n",
    "\n",
    "df.to_csv('data/Ch_En_Kor_sentences.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saves dictionaries to pickle\n",
    "import pickle\n",
    "\n",
    "with open('data/ch_word_to_sentencenum_dict.pkl', 'wb') as handle:\n",
    "    pickle.dump(ch_word_to_sentencenum_dict, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "with open('data/num_CEK60k_sentence_dict.pkl', 'wb') as handle:\n",
    "    pickle.dump(num_CEK60k_sentence_dict, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
